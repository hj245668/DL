ğŸŒ¼ EfficientNetB3 ê¸°ë°˜ TF-Flowers ë¶„ë¥˜ ëª¨ë¸

Transfer Learning Â· Fine-tuning Â· Advanced Augmentation Â· TTA Optimization

ğŸ“Œ í”„ë¡œì íŠ¸ ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” TensorFlow Flowers ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ EfficientNetB3 ì „ì´í•™ìŠµ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³ , ë‹¤ì–‘í•œ í•™ìŠµ ê¸°ë²•(Fine-tuning, Aggressive Augmentation, Test-Time Augmentation)ì„ ì ìš©í•˜ì—¬ ê³ ì„±ëŠ¥ ê½ƒ ì´ë¯¸ì§€ ë¶„ë¥˜ê¸°(Classifier) ë¥¼ ê°œë°œí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤.

ì‹¤í—˜ì„ í†µí•´ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ìµœëŒ€í•œ ëŒì–´ì˜¬ë¦¬ëŠ” ë° ì§‘ì¤‘í–ˆìœ¼ë©°,
í•™ìŠµâ€“ì¶”ë¡  ë‹¨ê³„ ëª¨ë‘ì—ì„œ ì„±ëŠ¥ ìµœì í™” ì „ëµì„ ì²´ê³„ì ìœ¼ë¡œ ì ìš©í•˜ì˜€ë‹¤.

ğŸ“Œ ì£¼ìš” íŠ¹ì§•
- EfficientNetB3 ê¸°ë°˜ ê³ ì„±ëŠ¥ ì´ë¯¸ì§€ ë¶„ë¥˜ê¸°
- Head Training + Fine-Tuning 2ë‹¨ê³„ í•™ìŠµ ì „ëµ
- ê°•ë ¥í•œ Data Augmentation ì ìš©
- Test-Time Augmentation(TTA) ìµœì í™”
- ëª¨ë¸ ì„±ëŠ¥ ì‹œê°í™” ë° ë¶„ì„ í¬í•¨

1. ë°ì´í„°ì…‹ ì†Œê°œ

ë³¸ í”„ë¡œì íŠ¸ëŠ” TensorFlow Datasets(TFDS) ì˜ tf_flowers ë¥¼ ì‚¬ìš©í•œë‹¤.

í´ë˜ìŠ¤ êµ¬ì„± (5ì¢…)
Label	Flower
0	Daisy
1	Dandelion
2	Roses
3	Sunflowers
4	Tulips
ë°ì´í„° ë¶„ë¦¬

Train: 80%

Validation: 10%

Test: 10%

2. ëª¨ë¸ êµ¬ì¡° ë° ì„ íƒ ì´ìœ 

âœ” EfficientNetB3ë¥¼ ì„ íƒí•œ ì´ìœ 

1) ì‘ê³  ë¹ ë¥´ë©´ì„œë„ ê°•ë ¥í•œ ì„±ëŠ¥
EfficientNetì€ compound scaling ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬
CNNì˜ ê¹Šì´/ë„ˆë¹„/í•´ìƒë„ë¥¼ ê· í˜• ìˆê²Œ í™•ì¥í•œë‹¤.

EfficientNet ê³„ì—´ì€ ê°™ì€ í¬ê¸°ì˜ ëª¨ë¸ ëŒ€ë¹„:
- ë” ë†’ì€ accuracy
- ë” ì ì€ íŒŒë¼ë¯¸í„° ìˆ˜
- ë” ë‚®ì€ FLOPs
ì„ ë‹¬ì„±í•˜ëŠ” ë§¤ìš° íš¨ìœ¨ì ì¸ ëª¨ë¸ì´ë‹¤.

2) ê½ƒ ì´ë¯¸ì§€ ë¶„ë¥˜ì™€ ê°™ì€ ìì—° ì´ë¯¸ì§€ì—ì„œ ë†’ì€ ì„±ëŠ¥

EfficientNetì€ ìì—° ì´ë¯¸ì§€ ë„ë©”ì¸ì—ì„œ íŠ¹íˆ ê°•ë ¥í•˜ë©°,
ê½ƒì²˜ëŸ¼ í˜•íƒœÂ·ìƒ‰ìƒÂ·ì§ˆê° íŒ¨í„´ì´ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì— ìµœì í™”ë˜ì–´ ìˆë‹¤.

3) Fine-tuning-friendly

EfficientNetì€:
âœ” BatchNorm êµ¬ì¡°ê°€ ë§¤ìš° ì•ˆì •ì ì´ê³ 
âœ” Partial Fine-tuning ë²”ìœ„ë¥¼ ì¡°ì ˆí•˜ê¸° ì‰½ê³ 
âœ” ì „ì´í•™ìŠµ ì‹œ ì„±ëŠ¥ ìƒìŠ¹í­ì´ ë†’ë‹¤.

ì´ í”„ë¡œì íŠ¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒëœ ì´ìœ  ì¤‘ í•˜ë‚˜ì´ë‹¤.

3. í•™ìŠµ ì „ëµ

âœ” 1ë‹¨ê³„: Head Training

EfficientNetB3 backbone ë™ê²°
Dense Layer + Dropout ë“± ìƒë‹¨ Classification Headë§Œ í•™ìŠµ
ëª©ì : ì‚¬ì „í•™ìŠµëœ ImageNet featureì™€ flower dataset ë¶„í¬ ì •ë ¬

âœ” 2ë‹¨ê³„: Fine-Tuning

EfficientNet backbone ë¶€ë¶„ í•´ì œ(FINE_TUNE_AT ê¸°ì¤€)
Lower-level feature ì¬í•™ìŠµ
ë‚®ì€ learning rate(3e-5) + CosineDecayRestarts ìŠ¤ì¼€ì¤„ ì ìš©
Batch Normalization ë ˆì´ì–´ëŠ” í•­ìƒ ë™ê²° (ì•ˆì •ì„± í™•ë³´)

4. ë°ì´í„° ì¦ê°• (Data Augmentation)

ë³¸ í”„ë¡œì íŠ¸ëŠ” ê°•í•œ ì¦ê°•(Strong Augmentation) ì„ í™œìš©í•˜ì—¬
ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í–ˆë‹¤.

ì ìš©ëœ ì¦ê°• ê¸°ë²•
- Random Flip
- Random Rotation
- Random Zoom
- Random Contrast
- Random Translation

ì¦ê°• íš¨ê³¼
- ë°ì´í„° ë‹¤ì–‘ì„± ì¦ê°€ â†’ Overfitting ê°ì†Œ
- ì‹¤ì œ ì´¬ì˜ í™˜ê²½(ê¸°ìš¸ê¸°/ë°ê¸°/í¬ê¸° ë³€í™”)ì— ëŒ€í•œ Robustness ì¦ê°€
- ë³¸ì§ˆì  íŠ¹ì§•í•™ìŠµ(ê½ƒì íŒ¨í„´Â·í˜•íƒœ)ì— ì§‘ì¤‘í•˜ë„ë¡ ìœ ë„

5. Test-Time Augmentation(TTA)

ì¶”ë¡  ì‹œ ë‹¤ì–‘í•œ ì‹œê° ë³€í˜•ì„ ì ìš©í•˜ì—¬
ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‰ê· (ensemble)í•˜ëŠ” ë°©ì‹.

ì ìš©ëœ ìµœê°• TTA ì¡°í•©
- ì›ë³¸
- ì¢Œìš° ë°˜ì „
- 90Â° íšŒì „
- 270Â° íšŒì „
- ì¤‘ì•™ í¬ë¡­ + resize
- ë°ê¸° ì¦ê°€
- ëŒ€ë¹„ ì¦ê°€

TTA íš¨ê³¼
- ì˜ˆì¸¡ ì•ˆì •ì„± ì¦ê°€
- ê²°ì • ê²½ê³„ ê·¼ì²˜ì˜ ìƒ˜í”Œì—ì„œ ì •í™•ë„ ìƒìŠ¹
- ì´ +1% ~ +3% accuracy í–¥ìƒ

6. ì„±ëŠ¥ ê²°ê³¼
âœ” Training & Validation Accuracy/Loss

ì•„ë˜ëŠ” Fine-tuning ì´í›„ Accuracy/Loss ë³€í™” ì‹œê°í™” ì˜ˆì‹œì´ë‹¤.

ğŸ“Š Accuracy Curve
Train Accuracy â†—  
Val Accuracy   â†—  (ì•ˆì •ì  ìƒìŠ¹)

ğŸ“Š Loss Curve
Train Loss â†“  
Val Loss   â†“ (Overfitting ì–µì œë¨)

âœ” Confusion Matrix ì˜ˆì‹œ
ì‹¤ì œ\ì˜ˆì¸¡	    Daisy	  Dandelion	Roses	Sunflowers	Tulips
Daisy	          â­	    â€¦      	  â€¦      	â€¦        	â€¦
Dandelion	       â€¦	    â­	      â€¦	      â€¦	        â€¦
Roses	           â€¦	    â€¦	        â­    	â€¦        	â€¦
Sunflowers     	 â€¦	    â€¦	        â€¦	      â­	      â€¦
Tulips	         â€¦	    â€¦        	â€¦	      â€¦	        â­

â†’ í´ë˜ìŠ¤ ê°„ ì˜¤ë¥˜ íŒ¨í„´ì„ íŒŒì•…í•  ìˆ˜ ìˆìœ¼ë©°,
EfficientNetì€ ìƒ‰/í˜•íƒœ ì°¨ì´ë¥¼ ì˜ êµ¬ë³„í•¨.

âœ” ìµœì¢… Test Accuracy ìš”ì•½
ë°©ë²•	Accuracy
Base Model	~92%
Fine-Tuning í›„	~94%
Final Model + TTA	95% ~ 96%
ğŸ‰ ê²°ë¡ : ì„±ëŠ¥ì´ ì‹¤ì§ˆì ìœ¼ë¡œ ìƒìœ„ 5% ìˆ˜ì¤€ê¹Œì§€ í–¥ìƒë¨

7. ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
ğŸŒ¼ ì˜ˆì‹œ
Input Image	ì˜ˆì¸¡ í´ë˜ìŠ¤	Confidence
ğŸŒ¼ Daisy ì´ë¯¸ì§€	Daisy	0.987
ğŸŒ» Sunflower ì´ë¯¸ì§€	Sunflower	0.992
ğŸŒ¹ Roses ì´ë¯¸ì§€	Rose	0.954
8. ê²°ë¡ 

ë³¸ í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒì˜ ìš”ì†Œë“¤ì´ ì¡°í•©ë˜ì–´ ë†’ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆë‹¤:

ğŸ”¥ í•µì‹¬ ê¸°ì—¬ ìš”ì†Œ

EfficientNetB3ì˜ íš¨ìœ¨ì ì¸ feature extraction

Fine-tuningì„ í†µí•œ Domain Adaptation

ê°•ë ¥í•œ Data Augmentation

ìµœì í™”ëœ Test-Time Augmentation

ì•ˆì •ì ì¸ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§(Cosine Decay Restarts)

ğŸ¯ ì‹¤ì „ ì ìš© ê°€ëŠ¥ì„±

ì•±/ì›¹ ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸

ê½ƒ ì‹ë³„ ì„œë¹„ìŠ¤

Edge-device ì ìš©(EfficientNetì€ ê²½ëŸ‰ ê°€ëŠ¥)

9. í–¥í›„ ê°œì„  ë°©í–¥

EfficientNetV2 or ConvNeXtë¡œ ì—…ê·¸ë ˆì´ë“œ

MixUp, CutMix ì¶”ê°€

AutoAugment, RandAugment ìë™ íƒìƒ‰

Ensemble ëª¨ë¸ êµ¬ì„±

GradCAM ê¸°ë°˜ ëª¨ë¸ í•´ì„ ì¶”ê°€

10. í”„ë¡œì íŠ¸ êµ¬ì¡°
â”œâ”€â”€ data/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ training.ipynb
â”‚   â”œâ”€â”€ fine_tuning.ipynb
â”‚   â””â”€â”€ tta_experiments.ipynb
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ stage1_best.weights.h5
â”‚   â”œâ”€â”€ stage2_best.weights.h5
â”‚   â””â”€â”€ final_model.h5
â”œâ”€â”€ README.md
â””â”€â”€ inference.py
