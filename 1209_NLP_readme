# Reuters 분류 실험 

실험 데이터 : TF-IDF 고차원 sparse 데이터

1. 파라미터 튜닝 결과
LightGBM
- 기본 (`num_leaves=31`, `max_depth=-1`): 0.356 
- 최적화(`num_leaves=150`, `max_depth=10`) : 0.814 (+128% up)

Gradient Boosting
- 기본 : 0.771 (준수한 성능)
- 최적화(`n_estimators=150`, `learning_rate=0.1`, `subsample=0.8`) : 0.004 (-99% ❌)
- 아마도 subsample + 높은 learning_rate로 인한 불안정성
- 주의 : 파라미터 변경 시 반드시 검증 필요

2. "Simple is Best" 
SVM
- 기본 : 0.8299 (1위)
- 최적화 : 0.8272 (여전히 1위)
- 학습시간 : 3-5초 (매우 빠름)

**왜 SVM이 승리했나?**
1. **선형 분류의 적합성**: TF-IDF는 이미 선형적으로 분리 가능한 고차원 공간
2. **정규화의 힘**: `C=0.5`, `class_weight='balanced'`가 과적합 방지
3. **복잡도 vs 성능**: 복잡한 모델(XGB, LGBM)보다 오히려 안정적

**실무 인사이트**:
- 텍스트 분류에서는 Linear SVM이 여전히 강력한 베이스라인
- 딥러닝이 만능은 아니다 (M12/M13 Dense가 SVM보다 낮음)
- **오컴의 면도날**: 단순한 모델로 충분하다면 복잡한 모델 불필요

---

## ⏱️ 3. 시간-성능 트레이드오프의 현실

### 효율성 분석

| 모델 | 정확도 | 시간 | 효율성 (Acc/Time) | 순위 |
|------|--------|------|-------------------|------|
| M4_SVM | 0.8290 | 4초 | 0.2073 | 👑 |
| M2_CNB | 0.7707 | 0.04초 | 19.27 | ⚡ |
| M1_NB | 0.6745 | 0.04초 | 16.86 | ⚡ |
| M7_GBM | 0.7720 | 1807초 | 0.0004 | 🐌 |
| M10_XGB | 0.8103 | 900초 | 0.0009 | 🐌 |

**핵심 발견**:
- SVM이 성능과 속도 모두에서 최적의 밸런스
- Naive Bayes는 속도는 빠르지만 성능 부족
- GBM/XGB는 성능 대비 시간 투자 가치 낮음

**실무 가이드**:
```
프로토타이핑 (1분 이내) → M4_SVM or M2_CNB
배포 환경 (빠른 예측) → M4_SVM
시간 충분 (오프라인 학습) → M10_XGB
```

---

## 🧠 4. 딥러닝이 항상 답은 아니다

### Dense Neural Network의 한계

**M12 Dense (기본)**:
- 기본 버전: 0.7934
- 최적화 버전: 0.7818 (-1.5%)
- BatchNorm, EarlyStopping 추가했지만 하락

**M13 Dense (고급)**:
- 더 깊은 네트워크(5층)로 설계
- 여전히 SVM(0.827)에 못 미침

**왜 딥러닝이 실패했나?**
1. **데이터 크기**: 8,982개 샘플은 딥러닝에 충분하지 않음
2. **특성의 성질**: TF-IDF는 이미 좋은 특성, 비선형 변환 불필요
3. **과적합**: Dropout, BatchNorm으로도 과적합 방지 어려움

**중요한 교훈**:
- 딥러닝은 대용량 데이터(100K+ 샘플)에서 빛을 발함
- 소규모 텍스트 분류는 전통적 ML이 더 효과적
- "최신 = 최고"는 오해

---

## 📊 5. Vocabulary 크기의 미미한 영향

### 5000 vs 10000 비교

| 모델 | Voca 5000 | Voca 10000 | 차이 |
|------|-----------|------------|------|
| M4_SVM | 0.8290 | 0.8299 | +0.09% |
| M10_XGB | 0.8068 | 0.8103 | +0.35% |
| M11_LGBM | 0.3544 | 0.3598 | +0.54% |

**발견**:
- Vocabulary 크기를 2배 늘려도 성능 향상은 1% 미만
- TF-IDF의 `max_features=5000` 제한이 이미 충분
- **실무 의미**: 메모리/속도를 위해 vocab 크기 제한 가능

---

## 🔄 6. 앙상블의 가치와 한계

### M8_Voting (CNB + LR + SVM)

**성능**:
- 기본 버전: 0.8085
- 최적화 버전: 0.8054
- 단독 최고(M4_SVM 0.8299)보다 낮음

**왜 앙상블이 최고가 아닌가?**
1. **다양성 부족**: SVM이 너무 강력해서 다른 모델의 기여도 낮음
2. **약한 모델 포함**: CNB, LR이 SVM보다 약해서 오히려 성능 저하
3. **soft voting 한계**: 확률 평균이 항상 최선은 아님

**더 나은 앙상블 전략**:
```python
# ✗ 나쁜 예: 약한 모델 포함
VotingClassifier([CNB, LR, SVM])

# ✓ 좋은 예: 비슷한 수준의 다양한 모델
VotingClassifier([SVM, XGB, LGBM], weights=[1, 2, 2])
```

---

## 🎓 7. 최적화 전략의 우선순위

### 효과 큰 최적화 (ROI 높음)

1. **LightGBM**: `num_leaves`, `max_depth` 조정 → +128%
2. **Naive Bayes**: `alpha` 튜닝 → +7~8%
3. **SVM**: `C` 값, `class_weight` → +1~2%

### 효과 없거나 역효과 (시간 낭비)

1. **GBM**: 파라미터 변경으로 오히려 폭망 (-99%)
2. **Dense**: BatchNorm, EarlyStopping 추가해도 하락 (-1.5%)
3. **Decision Tree**: 과도한 제약으로 성능 하락 (-38%)

**최적화 우선순위**:
```
1순위: 기본 설정이 명백히 잘못된 모델 (LGBM)
2순위: 단순 파라미터로 큰 효과 (NB alpha, SVM C)
3순위: 복잡한 튜닝 (XGB 정규화, 샘플링)
❌ 피하기: 검증 없는 무작정 최적화 (GBM)
```

---

## 💡 8. 실무 의사결정 프레임워크

### 시나리오별 모델 선택

#### 시나리오 1: 스타트업 MVP (빠른 출시)
```
추천: M4_SVM
이유: 3-5초 학습, 0.83 정확도, 안정적
구현: sklearn LinearSVC 기본 설정
```

#### 시나리오 2: 대기업 프로덕션 (최고 성능)
```
추천: M11_LGBM (최적화 버전)
이유: 0.815 정확도, 6-7분 학습 가능
주의: 반드시 파라미터 튜닝 필요
```

#### 시나리오 3: 실시간 예측 (저지연)
```
추천: M2_CNB
이유: 0.04초 학습, 0.77 정확도
보완: 모델 앙상블로 정확도 향상
```

#### 시나리오 4: 연구/실험 (성능 한계 탐색)
```
추천: M10_XGB + M11_LGBM 조합
이유: 그리드 서치로 최대 성능 추구
시간: 몇 시간 소요 감수
```

---

## 🚨 9. 위험 신호와 대응 방안

### 경고 신호 체크리스트

**⚠️ 경고 1**: 기본 설정에서 성능 < 0.5
- **의미**: 파라미터가 데이터와 맞지 않음
- **대응**: 파라미터 전면 재검토 (LGBM 사례)

**⚠️ 경고 2**: 최적화 후 성능 급락
- **의미**: 파라미터 설정 오류 또는 과적합
- **대응**: 기본 설정으로 롤백, 단계적 튜닝 (GBM 사례)

**⚠️ 경고 3**: 학습 시간 >> 예상 시간
- **의미**: 비효율적 알고리즘 또는 파라미터
- **대응**: early stopping, n_estimators 감소

**⚠️ 경고 4**: Train/Test 성능 차이 > 10%
- **의미**: 과적합
- **대응**: 정규화 강화, 데이터 증강

---

## 🎯 10. 재현 가능한 레시피

### 검증된 설정 (Copy-Paste Ready)

```python
# 🏆 Best Overall: Linear SVM
from sklearn.svm import LinearSVC

model = LinearSVC(
    C=0.5,                    # 정규화
    loss='squared_hinge',
    dual=False,              # n_samples > n_features
    max_iter=2000,
    class_weight='balanced'  # 불균형 처리
)

# ⚡ Best Speed: Complement Naive Bayes
from sklearn.naive_bayes import ComplementNB

model = ComplementNB(
    alpha=0.5,    # smoothing
    norm=True     # 정규화
)

# 🚀 Best Tuned: LightGBM
import lightgbm as lgb

model = lgb.LGBMClassifier(
    n_estimators=200,
    num_leaves=150,       # 중요!
    max_depth=10,         # 중요!
    learning_rate=0.05,
    min_child_samples=10,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=0.5,
    random_state=42
)

# ❌ 피해야 할 설정
# GBM의 subsample=0.8 + learning_rate=0.1 조합 (실패 사례)
```

---

## 📈 11. 성능 개선 로드맵

### 단계별 접근

**Phase 1: 베이스라인 (1일)**
```
1. Linear SVM 기본 설정
2. TF-IDF 기본 설정
→ 목표: 0.80+ 정확도
```

**Phase 2: 빠른 최적화 (3일)**
```
1. SVM C 값 튜닝
2. TF-IDF ngram, min_df 조정
3. Naive Bayes alpha 튜닝
→ 목표: 0.82+ 정확도
```

**Phase 3: 고급 최적화 (1주)**
```
1. LGBM 파라미터 그리드 서치
2. XGBoost 정규화 튜닝
3. 앙상블 구성
→ 목표: 0.83+ 정확도
```

**Phase 4: 한계 돌파 (2주+)**
```
1. 전처리 개선 (불용어, 어간 추출)
2. 특성 공학 (n-gram, TF-IDF 변형)
3. 딥러닝 시도 (BERT 등)
→ 목표: 0.85+ 정확도
```

---

## 🔬 12. 추가 실험 제안

### 시도해볼 가치가 있는 것들

1. **특성 선택**
   - Chi-square 통계로 중요 특성만 선택
   - TF-IDF 대신 TF or Count Vectorizer
   - Bigram, Trigram 실험

2. **앙상블 개선**
   - Stacking (메타 러너 추가)
   - Blending (홀드아웃 세트 활용)
   - 성능 비슷한 모델만 앙상블

3. **전처리 강화**
   - Lemmatization vs Stemming
   - 불용어 사전 최적화
   - 희귀 단어 제거 임계값 조정

4. **데이터 증강**
   - Back-translation
   - Synonym replacement
   - SMOTE for 불균형 클래스

---

## 🎓 최종 결론

### 3줄 요약
1. **Simple beats Complex**: Linear SVM이 복잡한 모델들을 이김
2. **Tuning is Critical**: LightGBM은 튜닝 필수, 안 하면 쓸모없음
3. **Time Matters**: 성능 1% 향상에 시간 10배는 과투자

### 실무자를 위한 골든 룰
```
✓ 항상 Linear SVM으로 시작하라
✓ 기본 설정을 맹신하지 마라 (특히 LGBM)
✓ 최적화 전에 검증 파이프라인을 만들어라
✓ 성능보다 안정성이 우선이다
✓ 딥러닝은 마지막 카드로 남겨라
```

### 마지막 인사이트
> "가장 정교한 알고리즘이 아니라, 가장 적합한 알고리즘이 승리한다."

이 실험은 **최신 기술 ≠ 최적 솔루션**임을 명확히 보여줍니다. 
문제의 특성(텍스트 분류, 중소규모 데이터)에 맞는 도구를 선택하는 것이 
무작정 복잡한 모델을 쓰는 것보다 훨씬 중요합니다.
