{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hj245668/DL/blob/main/1211_lyricsGen_cpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5281ddab",
      "metadata": {
        "id": "5281ddab"
      },
      "source": [
        "# 한글 가사 생성기 만들기 튜토리얼 (TensorFlow + LSTM)\n",
        "\n",
        "**가사 데이터를 기반으로 가사 생성**을 진행하는 과정을 정리하였습니다.\n",
        "\n",
        "1. 환경 설정 및 라이브러리 불러오기  \n",
        "2. 데이터 준비 (텍스트 파일에서 한 줄씩 읽어오기)  \n",
        "3. 전처리: `<start>`, `<end>` 토큰 추가 및 정규식 처리  \n",
        "4. 토크나이저로 정수 시퀀스 만들기 + 패딩 처리  \n",
        "5. 학습/검증 데이터셋 분리 및 `tf.data.Dataset` 구성  \n",
        "6. LSTM 기반 `TextGenerator` 모델 정의  \n",
        "7. 모델 학습 (`model.fit`)  \n",
        "8. 가사 생성 함수 구현 및 테스트  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8368a1b8",
      "metadata": {
        "id": "8368a1b8"
      },
      "source": [
        "## 1. 환경 설정 및 라이브러리 불러오기\n",
        "\n",
        "이 단계에서는 가사 생성 모델을 학습하는 데 필요한 기본 라이브러리를 불러옵니다.\n",
        "\n",
        "- `os`: 파일/폴더 경로 처리\n",
        "- `re`: 정규표현식(텍스트 전처리)\n",
        "- `numpy`: 수치 계산\n",
        "- `tensorflow`: 딥러닝 라이브러리 (LSTM, Embedding, Dataset 등)\n",
        "\n",
        "> 만약 로컬 환경에서 실행하는데 `tensorflow`가 설치되어 있지 않다면,  \n",
        "> 터미널 또는 노트북 셀에서 `pip install tensorflow` 명령을 먼저 실행해야 합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "247bf825",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "247bf825",
        "outputId": "28a172f2-ac0d-4b3e-d217-3d34e095ef3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7089b65",
      "metadata": {
        "id": "c7089b65"
      },
      "source": [
        "## 2. 데이터 준비: 텍스트 파일에서 한 줄씩 읽어오기\n",
        "\n",
        "이 예제에서는 **`data/lyrics/` 폴더 아래에 여러 개의 txt 파일**이 있고,  \n",
        "각 파일이 다음과 같은 형태라고 가정합니다.\n",
        "\n",
        "```text\n",
        "첫 번째 노래의 가사 1줄\n",
        "첫 번째 노래의 가사 2줄\n",
        "\n",
        "공백 줄은 자동으로 제거됩니다.\n",
        "```\n",
        "\n",
        "즉, **한 줄 = 한 문장(또는 한 소절)** 구조입니다.\n",
        "\n",
        "coding\n",
        "\n",
        "1. `data/lyrics` 폴더 안의 `.txt` 파일을 모두 찾음  \n",
        "2. 각 파일을 열어서 `read().splitlines()`로 줄 단위로 나눔  \n",
        "3. 공백 줄(`\"\"` 또는 공백만 있는 줄)은 제거  \n",
        "4. 모든 줄을 하나의 리스트 `raw_corpus`에 모음  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drkKvbZ3UHXy",
        "outputId": "363c80a4-4a2c-4204-fe1e-eca0e99dd3ae"
      },
      "id": "drkKvbZ3UHXy",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e07f981c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e07f981c",
        "outputId": "3925a886-ada1-4477-e6a7-d87597d9d036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 문장(줄) 개수: 32777\n",
            "\n",
            "[샘플 5줄 미리 보기]\n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "All:\n",
            "Speak, speak.\n",
            "First Citizen:\n"
          ]
        }
      ],
      "source": [
        "# 사용자가 자신의 환경에 맞게 바꿔야 하는 부분: 가사 데이터가 있는 폴더 경로\n",
        "data_dir = \"/content/drive/MyDrive/SQL_modu/1211_ML\"\n",
        "\n",
        "raw_corpus = []\n",
        "\n",
        "if not os.path.isdir(data_dir):\n",
        "    print(f\"경고: '{data_dir}' 폴더가 존재하지 않습니다. 경로를 수정하세요.\")\n",
        "else:\n",
        "    for fname in os.listdir(data_dir):\n",
        "        if not fname.endswith(\".txt\"):\n",
        "            continue\n",
        "        file_path = os.path.join(data_dir, fname)\n",
        "        with open(file_path, encoding=\"utf-8\") as f:\n",
        "            lines = f.read().splitlines()\n",
        "            # 양쪽 공백 제거 후, 완전히 빈 줄은 제외\n",
        "            lines = [line.strip() for line in lines if line.strip() != \"\"]\n",
        "            raw_corpus.extend(lines)\n",
        "\n",
        "    print(\"전체 문장(줄) 개수:\", len(raw_corpus))\n",
        "    print(\"\\n[샘플 5줄 미리 보기]\\n\")\n",
        "    for line in raw_corpus[:5]:\n",
        "        print(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a88318e",
      "metadata": {
        "id": "2a88318e"
      },
      "source": [
        "## 3. 전처리: `<start>`, `<end>` 토큰 추가 및 정규식 처리\n",
        "\n",
        "딥러닝 모델에 텍스트를 넣기 전에 **전처리(preprocessing)**를 해 줍니다.\n",
        "\n",
        "여기서 하는 주요 작업은:\n",
        "\n",
        "1. 양쪽 공백 제거 (`strip`)  \n",
        "2. `? . ! ,` 등의 문장부호 앞뒤에 공백을 넣어 토큰 분리가 쉬워지도록 함  \n",
        "3. 여러 개의 공백을 하나로 줄이기  \n",
        "4. **허용된 문자만 남기고 나머지는 공백으로 치환**  \n",
        "   - 한글: `가-힣`  \n",
        "   - 영문: `a-zA-Z`  \n",
        "   - 숫자: `0-9`  \n",
        "   - 기본 문장부호: `? . ! ,`  \n",
        "5. 문장 앞뒤에 `<start>`, `<end>` 토큰을 붙여서 **문장의 시작과 끝을 명시**\n",
        "\n",
        "이렇게 하면, 나중에 모델이 **문장이 어디서 시작해서 어디서 끝나는지**를 배울 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b8ac7005",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8ac7005",
        "outputId": "b1743b36-9ea9-4abd-9e6d-011d942b4544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리 후 문장 수: 32777\n",
            "\n",
            "[전처리된 문장 5개]\n",
            "\n",
            "<start> First Citizen <end>\n",
            "<start> Before we proceed any further , hear me speak . <end>\n",
            "<start> All <end>\n",
            "<start> Speak , speak . <end>\n",
            "<start> First Citizen <end>\n"
          ]
        }
      ],
      "source": [
        "def preprocess_sentence_ko(sentence: str) -> str:\n",
        "    # 1) 양쪽 공백 제거\n",
        "    sentence = sentence.strip()\n",
        "    # 2) 문장부호 앞뒤로 공백 추가\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    # 3) 여러 공백을 하나로 통일\n",
        "    sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
        "    # 4) 허용되지 않는 문자는 공백으로 치환\n",
        "    #    한글(가-힣), 영문, 숫자, 일부 문장부호만 남긴다.\n",
        "    sentence = re.sub(r\"[^0-9a-zA-Z가-힣?.!,]+\", \" \", sentence)\n",
        "    # 5) 다시 양쪽 공백 제거\n",
        "    sentence = sentence.strip()\n",
        "    # 6) 시작/끝 토큰 추가\n",
        "    sentence = \"<start> \" + sentence + \" <end>\"\n",
        "    return sentence\n",
        "\n",
        "\n",
        "# 전처리 적용\n",
        "corpus = [preprocess_sentence_ko(s) for s in raw_corpus]\n",
        "\n",
        "print(\"전처리 후 문장 수:\", len(corpus))\n",
        "print(\"\\n[전처리된 문장 5개]\\n\")\n",
        "for line in corpus[:5]:\n",
        "    print(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ae0ce97",
      "metadata": {
        "id": "2ae0ce97"
      },
      "source": [
        "## 4. 토크나이저로 정수 시퀀스 만들기 + 패딩 처리\n",
        "\n",
        "신경망은 텍스트 문자열을 직접 처리할 수 없으므로, **각 단어를 정수 ID로 바꾸는 과정**이 필요합니다.\n",
        "\n",
        "### 주요 개념\n",
        "\n",
        "- `Tokenizer`  \n",
        "  - 단어 → 정수, 정수 → 단어 매핑을 관리하는 도구입니다.\n",
        "  - `num_words`: 사용할 최대 단어 수 (자주 등장하는 단어부터 순서대로 사용)\n",
        "  - `oov_token`: 사전에 없는(out-of-vocabulary) 단어가 나왔을 때 대신 사용할 토큰\n",
        "\n",
        "- `texts_to_sequences`  \n",
        "  - 각 문장을 `[정수, 정수, 정수, ...]` 형태의 리스트로 변환\n",
        "\n",
        "- `pad_sequences`  \n",
        "  - 문장별 길이가 다르기 때문에, 모델에 넣기 위해 **모든 문장을 같은 길이로 맞추는 작업**입니다.\n",
        "  - `maxlen`보다 짧은 문장은 뒤를 0으로 채우고, 더 긴 문장은 잘라냅니다.\n",
        "\n",
        "아래 코드에서는:\n",
        "\n",
        "1. `Tokenizer`를 정의하고, `corpus`에 대해 `fit_on_texts`를 수행  \n",
        "2. `texts_to_sequences`로 정수 시퀀스 생성  \n",
        "3. `pad_sequences`로 길이를 `maxlen`으로 맞춤  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a64cbee9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a64cbee9",
        "outputId": "a455daa6-5ed5-4ce5-dcab-9ea1403b8eb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "텐서(shape): (32777, 30)\n",
            "vocab size(단어 수): 11464\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 사용할 최대 단어 수\n",
        "NUM_WORDS = 12000\n",
        "# 문장 최대 길이 (데이터에 따라 조정 가능)\n",
        "MAX_LEN = 30\n",
        "\n",
        "def tokenize(corpus):\n",
        "    tokenizer = Tokenizer(\n",
        "        num_words=NUM_WORDS,\n",
        "        filters=\" \",        # 이미 정규식으로 전처리를 했기 때문에 단순 공백만 필터\n",
        "        oov_token=\"<unk>\"\n",
        "    )\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)\n",
        "    tensor = pad_sequences(\n",
        "        tensor,\n",
        "        padding=\"post\",     # 뒤에서부터 0으로 채움\n",
        "        maxlen=MAX_LEN,\n",
        "    )\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)\n",
        "\n",
        "print(\"텐서(shape):\", tensor.shape)\n",
        "print(\"vocab size(단어 수):\", len(tokenizer.word_index) + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2f5685d",
      "metadata": {
        "id": "b2f5685d"
      },
      "source": [
        "## 5. 학습/검증 데이터셋 분리 및 `tf.data.Dataset` 구성\n",
        "\n",
        "모델의 성능을 객관적으로 평가하려면 **훈련에 사용하지 않은 데이터(검증 데이터)**가 필요합니다.\n",
        "\n",
        "여기에서는 간단하게:\n",
        "\n",
        "- 앞쪽 80%: 학습용\n",
        "- 뒤쪽 20%: 검증용\n",
        "\n",
        "으로 나누겠습니다.\n",
        "\n",
        "또한, 시퀀스 모델을 학습하기 위해 **입력과 라벨을 한 토큰씩 밀어서 구성**합니다.\n",
        "\n",
        "- 입력(`decoder_input`): `[w1, w2, w3, ..., w_{n-1}]`  \n",
        "- 라벨(`decoder_label`): `[w2, w3, w4, ..., w_n]`  \n",
        "\n",
        "이렇게 하면, 모델이 “현재까지 등장한 단어들을 보고 다음 단어를 예측”하도록 학습됩니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "41eddc4f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41eddc4f",
        "outputId": "c051f8aa-6f8f-45fb-c4b9-78bad3533ec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 dec/label: (26221, 29) (26221, 29)\n",
            "검증 dec/label: (6556, 29) (6556, 29)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<_BatchDataset element_spec=(TensorSpec(shape=(64, 29), dtype=tf.int32, name=None), TensorSpec(shape=(64, 29), dtype=tf.int32, name=None))>,\n",
              " <_BatchDataset element_spec=(TensorSpec(shape=(64, 29), dtype=tf.int32, name=None), TensorSpec(shape=(64, 29), dtype=tf.int32, name=None))>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# 데이터 섞기\n",
        "np.random.seed(42)\n",
        "indices = np.arange(tensor.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "tensor = tensor[indices]\n",
        "\n",
        "# 80% 학습, 20% 검증\n",
        "split_at = int(tensor.shape[0] * 0.8)\n",
        "train_seq = tensor[:split_at]\n",
        "val_seq = tensor[split_at:]\n",
        "\n",
        "# 디코더 입력/라벨 분리\n",
        "# 입력: 마지막 토큰 제외\n",
        "# 라벨: 첫 번째 토큰 제외\n",
        "dec_train = train_seq[:, :-1]\n",
        "label_train = train_seq[:, 1:]\n",
        "dec_val = val_seq[:, :-1]\n",
        "label_val = val_seq[:, 1:]\n",
        "\n",
        "print(\"학습 dec/label:\", dec_train.shape, label_train.shape)\n",
        "print(\"검증 dec/label:\", dec_val.shape, label_val.shape)\n",
        "\n",
        "# tf.data.Dataset 구성\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((dec_train, label_train))\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((dec_val, label_val))\n",
        "val_dataset = val_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "train_dataset, val_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9231ba8",
      "metadata": {
        "id": "c9231ba8"
      },
      "source": [
        "## 6. LSTM 기반 `TextGenerator` 모델 정의\n",
        "\n",
        "여기서는 가장 전형적인 **Embedding + LSTM 2층 + Dense** 구조를 사용합니다.\n",
        "\n",
        "구성은 다음과 같습니다.\n",
        "\n",
        "1. `Embedding`  \n",
        "   - 정수로 표현된 단어 ID → 밀집 벡터(임베딩)로 변환  \n",
        "   - 단어 간 의미/유사도를 벡터 공간에서 학습\n",
        "\n",
        "2. 첫 번째 `LSTM` (return_sequences=True)  \n",
        "   - 시퀀스를 입력받아 시퀀스를 출력 (각 타임스텝별 hidden state)\n",
        "\n",
        "3. 두 번째 `LSTM` (return_sequences=True)  \n",
        "   - 더 복잡한 패턴 학습, 상위 레벨 시퀀스 표현\n",
        "\n",
        "4. `Dense(vocab_size)`  \n",
        "   - 각 타임스텝에서 **전체 vocabulary 크기만큼의 로짓(logits)**을 출력  \n",
        "   - Softmax를 통해 다음 단어의 확률분포로 해석 가능\n",
        "\n",
        "> 여기서는 `tf.keras.Model`을 상속받아 `TextGenerator` 클래스를 만듭니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b944789a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b944789a",
        "outputId": "73ec9cba-1ac5-4394-b8a8-b0624216992d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델 출력 shape: (1, 29, 11464)\n"
          ]
        }
      ],
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x):\n",
        "        # x: (batch_size, seq_len)\n",
        "        out = self.embedding(x)   # (batch_size, seq_len, embedding_size)\n",
        "        out = self.rnn_1(out)     # (batch_size, seq_len, hidden_size)\n",
        "        out = self.rnn_2(out)     # (batch_size, seq_len, hidden_size)\n",
        "        out = self.linear(out)    # (batch_size, seq_len, vocab_size)\n",
        "        return out\n",
        "\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# 하이퍼파라미터 (필요에 따라 조정 가능)\n",
        "EMBEDDING_SIZE = 256\n",
        "HIDDEN_SIZE = 512\n",
        "\n",
        "model = TextGenerator(vocab_size, EMBEDDING_SIZE, HIDDEN_SIZE)\n",
        "\n",
        "# 더미 입력으로 모델 한 번 호출해서 구조 확인\n",
        "dummy_x = tf.zeros((1, MAX_LEN - 1), dtype=tf.int32)\n",
        "dummy_y = model(dummy_x)\n",
        "\n",
        "print(\"모델 출력 shape:\", dummy_y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7c29b6a",
      "metadata": {
        "id": "d7c29b6a"
      },
      "source": [
        "## 7. 모델 컴파일 및 학습\n",
        "\n",
        "### 손실 함수\n",
        "\n",
        "- `SparseCategoricalCrossentropy(from_logits=True)`  \n",
        "  - 다중 클래스 분류(여기서는 ‘다음 단어’ 예측)에 자주 사용되는 손실 함수입니다.\n",
        "  - `from_logits=True`로 설정하여 모델의 출력이 Softmax를 거치지 않은 **로짓(logits)**임을 명시합니다.\n",
        "\n",
        "### 옵티마이저\n",
        "\n",
        "- `Adam`  \n",
        "  - 학습 속도와 안정성이 좋아 NLP에서 널리 사용되는 옵티마이저입니다.\n",
        "\n",
        "### 학습 설정\n",
        "\n",
        "- `epochs`  \n",
        "  - 전체 데이터를 몇 번 반복해서 학습할지 정합니다.\n",
        "  - 데이터 양과 모델 크기에 따라 5~30 사이에서 조정해보면 좋습니다.\n",
        "\n",
        "> 주의: GPU가 없는 환경에서는 epoch 수를 너무 크게 하면 시간이 오래 걸릴 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9d3b9e60",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d3b9e60",
        "outputId": "1c30878a-e022-41cd-c01f-cf3737bf265b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2185s\u001b[0m 5s/step - loss: 2.5598 - val_loss: 1.6831\n",
            "Epoch 2/5\n",
            "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2190s\u001b[0m 5s/step - loss: 1.6288 - val_loss: 1.5773\n",
            "Epoch 3/5\n",
            "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2171s\u001b[0m 5s/step - loss: 1.5359 - val_loss: 1.5351\n",
            "Epoch 4/5\n",
            "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2468s\u001b[0m 6s/step - loss: 1.4684 - val_loss: 1.5087\n",
            "Epoch 5/5\n",
            "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2481s\u001b[0m 6s/step - loss: 1.4263 - val_loss: 1.4926\n"
          ]
        }
      ],
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction=\"none\",   # 각 타임스텝별 손실을 그대로 반환 (마스크 등 응용 가능)\n",
        ")\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "\n",
        "EPOCHS = 5  # 시작은 작게, 결과 보고 늘리는 것을 추천\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=EPOCHS\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "548d54dd",
      "metadata": {
        "id": "548d54dd"
      },
      "source": [
        "## 8. 가사 생성 함수 구현 및 테스트\n",
        "\n",
        "이제 학습된 모델을 사용해 **새로운 가사 문장**을 생성해 보겠습니다.\n",
        "\n",
        "전체 흐름은 다음과 같습니다.\n",
        "\n",
        "1. 시작 문장을 `<start>` 토큰과 함께 `tokenizer.texts_to_sequences`로 정수 시퀀스로 변환  \n",
        "2. 텐서로 바꿔서 모델에 입력  \n",
        "3. 모델이 출력한 로짓(logits)을 Softmax로 확률 분포로 바꾼 뒤,  \n",
        "   - 가장 확률이 높은 단어를 `argmax`로 선택 (단순 그리디 전략)  \n",
        "4. 선택한 단어를 입력 시퀀스 뒤에 붙이고, 다시 모델에 넣어 반복  \n",
        "5. `<end>` 토큰이 나오거나 최대 길이에 도달하면 종료  \n",
        "6. 정수 시퀀스를 다시 단어로 바꿔 하나의 문장으로 합침  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "07b11b48",
      "metadata": {
        "id": "07b11b48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "6780ba5b-e824-41b2-d967-532304f503f4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "cannot compute ConcatV2 as input #1(zero-based) was expected to be a int64 tensor but is a int32 tensor [Op:ConcatV2] name: concat",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-339131843.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# 예시: '사랑이란'으로 시작하는 문장 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m example = generate_text(\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-339131843.py\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model, tokenizer, init_sentence, max_len)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# 5) 선택한 단어를 시퀀스 뒤에 붙임\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         test_tensor = tf.concat(\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mtest_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicted_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6004\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6005\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6006\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute ConcatV2 as input #1(zero-based) was expected to be a int64 tensor but is a int32 tensor [Op:ConcatV2] name: concat"
          ]
        }
      ],
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=30):\n",
        "    # 1) 시작 문장을 시퀀스로 변환\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "\n",
        "    # 2) <end> 토큰 ID 가져오기\n",
        "    end_token = tokenizer.word_index.get(\"<end>\")\n",
        "    if end_token is None:\n",
        "        raise ValueError(\"<end> 토큰이 tokenizer에 없습니다. 전처리 과정을 확인하세요.\")\n",
        "\n",
        "    while True:\n",
        "        # 3) 모델 예측\n",
        "        predictions = model(test_tensor)  # (1, seq_len, vocab_size)\n",
        "        predictions = predictions[:, -1, :]  # 마지막 타임스텝의 예측만 사용\n",
        "\n",
        "        # 4) 확률분포로 변환 후 argmax로 단어 선택 (그리디 전략)\n",
        "        predicted_id = tf.argmax(tf.nn.softmax(predictions, axis=-1), axis=-1).numpy()[0]\n",
        "\n",
        "        # 5) 선택한 단어를 시퀀스 뒤에 붙임\n",
        "        test_tensor = tf.concat(\n",
        "            [test_tensor, tf.expand_dims([predicted_id], axis=0)],\n",
        "            axis=-1\n",
        "        )\n",
        "\n",
        "        # 6) 종료 조건: <end> 토큰이거나, max_len 도달\n",
        "        if predicted_id == end_token:\n",
        "            break\n",
        "        if test_tensor.shape[1] >= max_len:\n",
        "            break\n",
        "\n",
        "    # 7) 정수 시퀀스를 단어로 복원\n",
        "    generated = \"\"\n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        word = tokenizer.index_word.get(word_index, \"<unk>\")\n",
        "        generated += word + \" \"\n",
        "\n",
        "    return generated\n",
        "\n",
        "\n",
        "# 예시: '사랑이란'으로 시작하는 문장 생성\n",
        "example = generate_text(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    init_sentence=\"<start> i\",\n",
        "    max_len=30,\n",
        ")\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06b91c8e",
      "metadata": {
        "id": "06b91c8e"
      },
      "source": [
        "## 9. 마무리 및 커스터마이징 포인트\n",
        "\n",
        "이 노트북에서 구현한 것은 **한글 가사 데이터로 LSTM 기반 가사 생성기**를 만드는 기본 골격입니다.\n",
        "\n",
        "### 직접 바꿔보면 좋은 부분\n",
        "\n",
        "1. **데이터 경로 및 형식**\n",
        "   - `data_dir = \"data/lyrics\"` 부분을 자신의 폴더 구조에 맞게 수정\n",
        "   - 한 파일에 여러 곡이 들어있다면, 구분자를 기준으로 나눠서 `raw_corpus`를 구성해도 됩니다.\n",
        "\n",
        "2. **전처리 규칙 (`preprocess_sentence_ko`)**\n",
        "   - 이모지, 해시태그, 괄호 등을 살리고 싶다면 정규식 범위를 늘리면 됩니다.\n",
        "   - 줄 단위가 아니라 문장 단위로 나누고 싶다면 `.` 또는 `?` 기준으로 split해서 사용해도 됩니다.\n",
        "\n",
        "3. **모델 크기**\n",
        "   - `EMBEDDING_SIZE`, `HIDDEN_SIZE`를 늘리면 표현력은 좋아지지만 메모리와 시간이 더 필요합니다.\n",
        "   - 데이터가 적다면 너무 큰 모델은 과적합(overfitting)을 일으킬 수 있으니 주의하세요.\n",
        "\n",
        "4. **학습 설정**\n",
        "   - `EPOCHS` 수를 변경하면서 loss의 변화를 관찰해 보세요.\n",
        "   - EarlyStopping, ModelCheckpoint 등의 콜백을 추가하면 더 안정적인 학습이 가능합니다.\n",
        "\n",
        "5. **텍스트 생성 전략**\n",
        "   - 현재는 가장 확률이 높은 단어만 선택하는 **그리디(탐욕적) 전략**을 사용하고 있습니다.\n",
        "   - 더 다양한 문장을 얻고 싶다면 temperature sampling, top-k, top-p(nucleus) sampling 등을 적용해 볼 수 있습니다.\n",
        "\n",
        "---\n",
        "\n",
        "이 노트북을 기본 틀로 삼아서,  \n",
        "- 랩 가사, 발라드, 트롯, K-POP, 영어/일본어 등  \n",
        "원하는 스타일에 맞게 데이터를 넣고 튜닝해 보시면 됩니다.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}